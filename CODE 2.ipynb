{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba8576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (22.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git 'C:\\Users\\manab kumar jena\\AppData\\Local\\Temp\\pip-req-build-53ic9qxq'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepset-ai/haystack.git\n",
      "  Cloning https://github.com/deepset-ai/haystack.git to c:\\users\\manab kumar jena\\appdata\\local\\temp\\pip-req-build-53ic9qxq\n",
      "  Resolved https://github.com/deepset-ai/haystack.git to commit 0e83535108d0742735ecc4a8c8d93f555839a54b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting quantulum3\n",
      "  Downloading quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n",
      "     ---------------------------------------- 10.7/10.7 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.26.0-py3-none-any.whl (17.8 MB)\n",
      "     ---------------------------------------- 17.8/17.8 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (3.2.0)\n",
      "Collecting rapidfuzz\n",
      "  Using cached rapidfuzz-2.0.11-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.3.4)\n",
      "Collecting azure-ai-formrecognizer==3.2.0b2\n",
      "  Downloading azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
      "     -------------------------------------- 219.7/219.7 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.7.1)\n",
      "Collecting tika\n",
      "  Downloading tika-1.24.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers==4.19.2\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 4.6 MB/s eta 0:00:00\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     -------------------------------------- 981.5/981.5 kB 2.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting elasticsearch<=7.10,>=7.7\n",
      "  Using cached elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
      "Collecting azure-core<1.23\n",
      "  Downloading azure_core-1.22.1-py3-none-any.whl (178 kB)\n",
      "     -------------------------------------- 178.6/178.6 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers>=2.2.0\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "     ---------------------------------------- 79.7/79.7 kB 4.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (2.6.3)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 43.6/43.6 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch<1.12,>1.9\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "     -------------------------------------- 157.9/157.9 MB 2.9 MB/s eta 0:00:00\n",
      "Collecting posthog\n",
      "  Downloading posthog-1.4.8-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (2.26.0)\n",
      "Collecting elastic-apm\n",
      "  Downloading elastic-apm-6.9.1.tar.gz (164 kB)\n",
      "     -------------------------------------- 164.2/164.2 kB 3.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     -------------------------------------- 95.8/95.8 kB 684.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-docx in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (0.8.11)\n",
      "Requirement already satisfied: nltk in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (3.6.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (4.62.3)\n",
      "Collecting mmh3\n",
      "  Downloading mmh3-3.0.0-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.8.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (8.10.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack==1.4.1rc0) (1.16.0)\n",
      "Collecting msrest>=0.6.21\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.2/85.2 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting azure-common~=1.1\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (3.3.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 887.4 kB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.4/84.4 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (2021.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (1.20.3)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==1.4.1rc0) (1.26.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==1.4.1rc0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from requests->farm-haystack==1.4.1rc0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from requests->farm-haystack==1.4.1rc0) (3.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->farm-haystack==1.4.1rc0) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->farm-haystack==1.4.1rc0) (2.2.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 795.8 kB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from torch<1.12,>1.9->farm-haystack==1.4.1rc0) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from tqdm->farm-haystack==1.4.1rc0) (0.4.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (21.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (58.0.4)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.6.tar.gz (62 kB)\n",
      "     ---------------------------------------- 62.2/62.2 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (2021.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (0.3)\n",
      "Requirement already satisfied: Flask in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (1.1.2)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "     -------------------------------------- 210.7/210.7 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting protobuf>=3.7.0\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.1/904.1 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (2.0.0)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (1.4.22)\n",
      "Collecting waitress\n",
      "  Downloading waitress-2.1.1-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.3/57.3 kB ? eta 0:00:00\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.3/42.3 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.20.1-py3-none-any.whl (18 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "     -------------------------------------- 146.2/146.2 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (4.8.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (8.0.3)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.2/181.2 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from pandas->farm-haystack==1.4.1rc0) (2.8.2)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff<2.0.0,>=1.10.0\n",
      "  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from python-docx->farm-haystack==1.4.1rc0) (4.6.3)\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.6/101.6 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting inflect\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Downloading jarowinkler-1.0.2-cp39-cp39-win_amd64.whl (59 kB)\n",
      "     ---------------------------------------- 59.4/59.4 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow->farm-haystack==1.4.1rc0) (2.1.0)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.5/151.5 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting pywin32==227\n",
      "  Downloading pywin32-227-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "     ---------------------------------------- 9.1/9.1 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.3/54.3 kB ? eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.1/63.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow->farm-haystack==1.4.1rc0) (3.6.0)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.7/41.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.19.2->farm-haystack==1.4.1rc0) (3.0.4)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from sqlalchemy->mlflow->farm-haystack==1.4.1rc0) (1.1.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.0.2)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from prometheus-flask-exporter->mlflow->farm-haystack==1.4.1rc0) (0.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack==1.4.1rc0) (8.4.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask->mlflow->farm-haystack==1.4.1rc0) (1.1.1)\n",
      "Building wheels for collected packages: farm-haystack, sentence-transformers, elastic-apm, langdetect, seqeval, tika, databricks-cli, docopt\n",
      "  Building wheel for farm-haystack (pyproject.toml): started\n",
      "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for farm-haystack: filename=farm_haystack-1.4.1rc0-py3-none-any.whl size=548077 sha256=05d532eca4cc7b527947cd7e97b804aab3a179d80a9396feb01fc13deae30cde\n",
      "  Stored in directory: C:\\Users\\manab kumar jena\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kz8w9ia3\\wheels\\47\\fc\\da\\022dd81c208ff5db06a026f418c7060d75bda1eee7760fc577\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=6c4fda2b0273fe6b6006cc758e2cc0ddc14f3477416519d5c37c53f08ff14448\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\2b\\11\\3b\\32a18fb9f2253b25d3d1a06f0a84e2d516e7efa19c8c71a283\n",
      "  Building wheel for elastic-apm (pyproject.toml): started\n",
      "  Building wheel for elastic-apm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for elastic-apm: filename=elastic_apm-6.9.1-py3-none-any.whl size=323816 sha256=032ff7975757b77aa78c16a7fc92b520e16168a2dfc952b64888bec6da23d6e4\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\f0\\1c\\ca\\25b87485f072f61abdb5daf345be553d04e488a8e1f02cbdd2\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=381e50cb85ff8224177eb8f16907e5ed548e105b90432c33c21aacfd14866b21\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=8df9abf9f3c7c01f0f517cc4390c693578123cbf6d4c24348062d3e0264ee65f\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=34cdc6329ca58dd58abca6e4d575a6a875ce008e39e40604d93d864f7110b0cd\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\ec\\76\\38\\0e4b92d8a3a89cbfff5be03a40c02d15b2072b1b08ebf28d6a\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=436d4afff4568eb2f3cda5daa70b295ddbe16688f403b415972bd11d83e3518d\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\af\\e4\\ed\\c601b319f71b0c442f62feba2a97b37617b2bf9e3f9fd6190b\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=b3ccf61acf186052e236d1c18245cb427ff67fb9bde488ece4fed22b8294faa1\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built farm-haystack sentence-transformers elastic-apm langdetect seqeval tika databricks-cli docopt\n",
      "Installing collected packages: tokenizers, tabulate, sentencepiece, pywin32, monotonic, mmh3, docopt, azure-common, websocket-client, waitress, torch, sqlparse, smmap, querystring-parser, protobuf, oauthlib, num2words, Mako, langdetect, jarowinkler, isodate, inflect, elasticsearch, elastic-apm, dill, backoff, torchvision, tika, requests-oauthlib, rapidfuzz, quantulum3, posthog, huggingface-hub, gitdb, docker, databricks-cli, azure-core, alembic, transformers, seqeval, prometheus-flask-exporter, msrest, gitpython, sentence-transformers, mlflow, azure-ai-formrecognizer, farm-haystack\n",
      "  Attempting uninstall: pywin32\n",
      "    Found existing installation: pywin32 228\n",
      "    Uninstalling pywin32-228:\n",
      "      Successfully uninstalled pywin32-228\n",
      "Successfully installed Mako-1.2.0 alembic-1.7.7 azure-ai-formrecognizer-3.2.0b2 azure-common-1.1.28 azure-core-1.22.1 backoff-1.11.1 databricks-cli-0.16.6 dill-0.3.5.1 docker-5.0.3 docopt-0.6.2 elastic-apm-6.9.1 elasticsearch-7.10.0 farm-haystack-1.4.1rc0 gitdb-4.0.9 gitpython-3.1.27 huggingface-hub-0.6.0 inflect-5.6.0 isodate-0.6.1 jarowinkler-1.0.2 langdetect-1.0.9 mlflow-1.26.0 mmh3-3.0.0 monotonic-1.6 msrest-0.6.21 num2words-0.5.10 oauthlib-3.2.0 posthog-1.4.8 prometheus-flask-exporter-0.20.1 protobuf-3.20.1 pywin32-227 quantulum3-0.7.10 querystring-parser-1.2.4 rapidfuzz-2.0.11 requests-oauthlib-1.3.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 seqeval-1.2.2 smmap-5.0.0 sqlparse-0.4.2 tabulate-0.8.9 tika-1.24 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2 waitress-2.1.1 websocket-client-1.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepset-ai/haystack.git\n",
      "  Cloning https://github.com/deepset-ai/haystack.git to c:\\users\\manab kumar jena\\appdata\\local\\temp\\pip-req-build-z0b7monv\n",
      "  Resolved https://github.com/deepset-ai/haystack.git to commit 0e83535108d0742735ecc4a8c8d93f555839a54b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting elastic-apm\n",
      "  Using cached elastic-apm-6.9.1.tar.gz (164 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tika\n",
      "  Using cached tika-1.24.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pydantic in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.8.2)\n",
      "Collecting azure-core<1.23\n",
      "  Using cached azure_core-1.22.1-py3-none-any.whl (178 kB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (3.6.5)\n",
      "Collecting elasticsearch<=7.10,>=7.7\n",
      "  Using cached elasticsearch-7.10.0-py2.py3-none-any.whl (321 kB)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting rapidfuzz\n",
      "  Using cached rapidfuzz-2.0.11-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting torch<1.12,>1.9\n",
      "  Using cached torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: python-docx in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (0.8.11)\n",
      "Collecting transformers==4.19.2\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (3.2.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (8.10.0)\n",
      "Collecting sentence-transformers>=2.2.0\n",
      "  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (4.62.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.3.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (2.6.3)\n",
      "Collecting posthog\n",
      "  Using cached posthog-1.4.8-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (2.26.0)\n",
      "Collecting quantulum3\n",
      "  Using cached quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n",
      "Collecting mlflow\n",
      "  Using cached mlflow-1.26.0-py3-none-any.whl (17.8 MB)\n",
      "Collecting azure-ai-formrecognizer==3.2.0b2\n",
      "  Using cached azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from farm-haystack==1.4.1rc0) (1.0.2)\n",
      "Collecting mmh3\n",
      "  Using cached mmh3-3.0.0-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Collecting msrest>=0.6.21\n",
      "  Using cached msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack==1.4.1rc0) (1.16.0)\n",
      "Collecting azure-common~=1.1\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (21.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from transformers==4.19.2->farm-haystack==1.4.1rc0) (2021.8.3)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==1.4.1rc0) (1.26.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==1.4.1rc0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from requests->farm-haystack==1.4.1rc0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from requests->farm-haystack==1.4.1rc0) (3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->farm-haystack==1.4.1rc0) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->farm-haystack==1.4.1rc0) (1.1.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git 'C:\\Users\\manab kumar jena\\AppData\\Local\\Temp\\pip-req-build-z0b7monv'\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n",
      "    WARNING: No metadata found in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from torch<1.12,>1.9->farm-haystack==1.4.1rc0) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from tqdm->farm-haystack==1.4.1rc0) (0.4.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (58.0.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from jsonschema->farm-haystack==1.4.1rc0) (21.2.0)\n",
      "Requirement already satisfied: Flask in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (1.1.2)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Using cached prometheus_flask_exporter-0.20.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (2021.3)\n",
      "Collecting waitress\n",
      "  Using cached waitress-2.1.1-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (1.4.22)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (2.0.0)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (4.8.1)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Using cached databricks-cli-0.16.6.tar.gz (62 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting protobuf>=3.7.0\n",
      "  Using cached protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (8.0.3)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting alembic\n",
      "  Using cached alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from mlflow->farm-haystack==1.4.1rc0) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from pandas->farm-haystack==1.4.1rc0) (2.8.2)\n",
      "Collecting backoff<2.0.0,>=1.10.0\n",
      "  Using cached backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from python-docx->farm-haystack==1.4.1rc0) (4.6.3)\n",
      "Collecting num2words\n",
      "  Using cached num2words-0.5.10-py3-none-any.whl (101 kB)\n",
      "Collecting inflect\n",
      "  Using cached inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Using cached jarowinkler-1.0.2-cp39-cp39-win_amd64.whl (59 kB)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow->farm-haystack==1.4.1rc0) (2.1.0)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting pywin32==227\n",
      "  Using cached pywin32-227-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow->farm-haystack==1.4.1rc0) (3.6.0)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.19.2->farm-haystack==1.4.1rc0) (3.0.4)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from sqlalchemy->mlflow->farm-haystack==1.4.1rc0) (1.1.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Flask->mlflow->farm-haystack==1.4.1rc0) (2.0.2)\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from prometheus-flask-exporter->mlflow->farm-haystack==1.4.1rc0) (0.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack==1.4.1rc0) (8.4.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\manab kumar jena\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask->mlflow->farm-haystack==1.4.1rc0) (1.1.1)\n",
      "Building wheels for collected packages: farm-haystack, sentence-transformers, elastic-apm, langdetect, seqeval, tika, databricks-cli, docopt\n",
      "  Building wheel for farm-haystack (pyproject.toml): started\n",
      "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for farm-haystack: filename=farm_haystack-1.4.1rc0-py3-none-any.whl size=548077 sha256=d354e9b4e2de7e18e74b2c44d12e222b0eec9935859a6de47943341aceadf01f\n",
      "  Stored in directory: C:\\Users\\manab kumar jena\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-fgdc1au2\\wheels\\47\\fc\\da\\022dd81c208ff5db06a026f418c7060d75bda1eee7760fc577\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=3c12769f702a323ef5bce8b4b28c0c40968de598716b36d56dc0404587ac27ac\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\2b\\11\\3b\\32a18fb9f2253b25d3d1a06f0a84e2d516e7efa19c8c71a283\n",
      "  Building wheel for elastic-apm (pyproject.toml): started\n",
      "  Building wheel for elastic-apm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for elastic-apm: filename=elastic_apm-6.9.1-py3-none-any.whl size=323816 sha256=84e63362a0bcd71e0d97a56f47ea66ba918b0670a2130677aacc01f85be5dddf\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\f0\\1c\\ca\\25b87485f072f61abdb5daf345be553d04e488a8e1f02cbdd2\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=9e33d1c7938d792c7e66a6f72044ad54857d0edd922cda632f487cb1fc2966a9\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=5d8f18064a919f263bdab23bceb729530584c4c2d7730037b14d3d8e652016f8\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=f89a228f3f123758e0fc372be455817dc34088192f93fba4192ce903f7b52787\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\ec\\76\\38\\0e4b92d8a3a89cbfff5be03a40c02d15b2072b1b08ebf28d6a\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=46e309d1751145f6d28f4b0e8de8ab04fcb14983a82940022277c633bcc1f45e\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\af\\e4\\ed\\c601b319f71b0c442f62feba2a97b37617b2bf9e3f9fd6190b\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=e36694c688be3aa04753f2a5515c6de8ac37a28fc46fafd50d46fc41d93c3bf2\n",
      "  Stored in directory: c:\\users\\manab kumar jena\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built farm-haystack sentence-transformers elastic-apm langdetect seqeval tika databricks-cli docopt\n",
      "Installing collected packages: tokenizers, tabulate, sentencepiece, pywin32, monotonic, mmh3, docopt, azure-common, websocket-client, waitress, torch, sqlparse, smmap, querystring-parser, protobuf, oauthlib, num2words, Mako, langdetect, jarowinkler, isodate, inflect, elasticsearch, elastic-apm, dill, backoff, torchvision, tika, requests-oauthlib, rapidfuzz, quantulum3, posthog, huggingface-hub, gitdb, docker, databricks-cli, azure-core, alembic, transformers, seqeval, prometheus-flask-exporter, msrest, gitpython, sentence-transformers, mlflow, azure-ai-formrecognizer, farm-haystack\n",
      "  Attempting uninstall: pywin32\n",
      "    Found existing installation: pywin32 228\n",
      "    Can't uninstall 'pywin32'. No files were found to uninstall.\n",
      "Successfully installed Mako-1.2.0 alembic-1.7.7 azure-ai-formrecognizer-3.2.0b2 azure-common-1.1.28 azure-core-1.22.1 backoff-1.11.1 databricks-cli-0.16.6 dill-0.3.5.1 docker-5.0.3 docopt-0.6.2 elastic-apm-6.9.1 elasticsearch-7.10.0 farm-haystack-1.4.1rc0 gitdb-4.0.9 gitpython-3.1.27 huggingface-hub-0.6.0 inflect-5.6.0 isodate-0.6.1 jarowinkler-1.0.2 langdetect-1.0.9 mlflow-1.26.0 mmh3-3.0.0 monotonic-1.6 msrest-0.6.21 num2words-0.5.10 oauthlib-3.2.0 posthog-1.4.8 prometheus-flask-exporter-0.20.1 protobuf-3.20.1 pywin32-227 quantulum3-0.7.10 querystring-parser-1.2.4 rapidfuzz-2.0.11 requests-oauthlib-1.3.1 sentence-transformers-2.2.0 sentencepiece-0.1.96 seqeval-1.2.2 smmap-5.0.0 sqlparse-0.4.2 tabulate-0.8.9 tika-1.24 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2 waitress-2.1.1 websocket-client-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install git+https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c025416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "ERROR - root -  Failed to import 'magic' (from 'python-magic' and 'python-magic-bin' on Windows). FileTypeClassifier will not perform mimetype detection on extensionless files. Please make sure the necessary OS libraries are installed if you need this functionality.\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea77283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find deepset/roberta-base-squad2 locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38ab308390843169fd6e48364cda43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a10ae1ebb04ac6b7250c35903971aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MANABK~1\\AppData\\Local\\Temp/ipykernel_23704/2347763350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFARMReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFARMReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"deepset/roberta-base-squad2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# data_dir = \"PATH/TO_YOUR/TRAIN_DATA\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\nodes\\base.py\u001b[0m in \u001b[0;36mwrapper_exportable_to_yaml\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Call the actuall __init__ function with all the arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Create the configuration dictionary if it doesn't exist yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\nodes\\reader\\farm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_name_or_path, model_version, context_window_size, batch_size, use_gpu, devices, no_ans_boost, return_no_answer, top_k, top_k_per_candidate, top_k_per_sample, num_processes, max_seq_len, doc_stride, progress_bar, duplicate_filtering, use_confidence_scores, confidence_threshold, proxies, local_files_only, force_download, use_auth_token)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_k_per_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k_per_candidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         self.inferencer = QAInferencer.load(\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\infer.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, model_name_or_path, revision, batch_size, gpu, task_type, return_class_probs, strict, max_seq_len, doc_stride, extraction_strategy, extraction_layer, num_processes, disable_tqdm, tokenizer_class, use_fast, tokenizer_args, multithreading_rust, devices, use_auth_token, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m                 )\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             model = AdaptiveModel.convert_from_transformers(\n\u001b[0m\u001b[0;32m    198\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\model\\adaptive_model.py\u001b[0m in \u001b[0;36mconvert_from_transformers\u001b[1;34m(cls, model_name_or_path, device, revision, task_type, processor, use_auth_token, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mlm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLanguageModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;31m# Infer task type from config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\model\\language_model.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, pretrained_model_name_or_path, language, use_auth_token, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlanguage_model_class\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 language_model = cls.subclasses[language_model_class].load(\n\u001b[0m\u001b[0;32m    180\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\model\\language_model.py\u001b[0m in \u001b[0;36mquiet_from_pretrained_func\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mt_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_pretrained_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Restore the log level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\model\\language_model.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, pretrained_model_name_or_path, language, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;31m# Huggingface transformer Style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m             \u001b[0mroberta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[0mroberta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_or_infer_language_from_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mroberta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m                 \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[0;32m   1853\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m    283\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mhttp_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Downloading\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     )\n\u001b[1;32m--> 437\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# filter out keep-alive new chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m                 if (\n\u001b[0;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "data_dir = \"data/\"\n",
    "# data_dir = \"PATH/TO_YOUR/TRAIN_DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6659a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: data\\dev-v2.0.json \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 5 parallel workers to convert 1204 dictionaries to pytorch datasets (chunksize = 49)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /w\\   /w\\   /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /'\\   / \\   /'\\   /'\\   / \\ \n",
      "Preprocessing Dataset data\\dev-v2.0.json: 100%|████████████████████████████████| 1204/1204 [00:13<00:00, 91.17 Dicts/s]\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING DEV DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  No dev set is being loaded\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TEST DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  No test set is being loaded\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  DATASETS SUMMARY\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in train: 13600\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in dev  : 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in test : 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Total examples   : 13600\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Longest sequence length observed after clipping:     256\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Average sequence length after clipping: 174.0468382352941\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Proportion clipped:      0.12830882352941175\n",
      "INFO - haystack.modeling.model.optimization -  Loading optimizer `AdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
      "INFO - haystack.modeling.model.optimization -  Using scheduler 'get_linear_schedule_with_warmup'\n",
      "INFO - haystack.modeling.model.optimization -  Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 1360, 'num_warmup_steps': 272}'\n",
      "Train epoch 0/0 (Cur. train loss: 6.8378):   1%|▏                                   | 9/1360 [01:37<4:03:05, 10.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MANABK~1\\AppData\\Local\\Temp/ipykernel_23704/2136749591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dev-v2.0.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"my_model\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\nodes\\reader\\farm.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, caching, cache_path)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \"\"\"\n\u001b[1;32m--> 396\u001b[1;33m         return self._training_procedure(\n\u001b[0m\u001b[0;32m    397\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mtrain_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\nodes\\reader\\farm.py\u001b[0m in \u001b[0;36m_training_procedure\u001b[1;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, teacher_model, teacher_batch_size, caching, cache_path, distillation_loss_weight, distillation_loss, temperature, tinybert, processor)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;31m# 5. Let it grow!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferencer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\training\\base.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[1;31m# Move batch of samples to device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;31m# Perform  evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\training\\base.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, batch, step)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mper_sample_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits_to_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mper_sample_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\haystack\\modeling\\training\\base.py\u001b[0m in \u001b[0;36mbackward_propagate\u001b[1;34m(self, loss, step)\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_acc_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reader.train(data_dir=data_dir, train_filename=\"dev-v2.0.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48bed89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
